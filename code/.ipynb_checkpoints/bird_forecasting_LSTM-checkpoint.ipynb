{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a70621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model, save_model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessing import get_ids_and_files_in_dir, percentile_remove_outlier, MinMaxScaler, NormalDistributionScaler, binning_date_y\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c78f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self,\n",
    "                 training_set_dir,\n",
    "                 model_save_dir,\n",
    "                 output_dir=\".\",\n",
    "                 model_file_prefix='model',\n",
    "                 training_set_id_range=(0, np.Inf),\n",
    "                 training_set_length=3,\n",
    "                 scaler = 'mm',\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        :param training_set_dir: directory contains the training set files. File format: 76.csv\n",
    "        :param model_save_dir: directory to receive trained model and model weights. File format: model-76.json/model-weight-76.h5\n",
    "        :param model_file_prefix='model': file prefix for model file\n",
    "        :param training_set_range=(0, np.Inf): enterprise ids in this range (a, b) would be analyzed. PS: a must be less than b\n",
    "        :param training_set_length=3: first kth columns in training set file will be used as training set and the following one is expected value\n",
    "        :param train_test_ratio=3: the ratio of training set size to test set size when splitting input data\n",
    "        :param output_dir=\".\": output directory for prediction files\n",
    "        :param scaler: scale data set using - mm: MinMaxScaler, norm: NormalDistributionScaler\n",
    "        :param **kwargs: lstm_output_dim=4: output dimension of LSTM layer;\n",
    "                        activation_lstm='relu': activation function for LSTM layers;\n",
    "                        activation_dense='relu': activation function for Dense layer;\n",
    "                        activation_last='softmax': activation function for last layer;\n",
    "                        drop_out=0.2: fraction of input units to drop;\n",
    "                        np_epoch=25, the number of epoches to train the model. epoch is one forward pass and one backward pass of all the training examples;\n",
    "                        batch_size=100: number of samples per gradient update. The higher the batch size, the more memory space you'll need;\n",
    "                        loss='categorical_crossentropy': loss function;\n",
    "                        optimizer='rmsprop'\n",
    "        \"\"\"\n",
    "        self.training_set_dir = training_set_dir\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.model_file_prefix = model_file_prefix\n",
    "        self.training_set_id_range = training_set_id_range\n",
    "        self.training_set_length = training_set_length\n",
    "        self.output_dir = output_dir\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        if not os.path.exists(self.model_save_dir):\n",
    "            os.makedirs(self.model_save_dir)\n",
    "        self.scaler = scaler\n",
    "        self.test_size = kwargs.get('test_size', 0.2)\n",
    "        self.lstm_output_dim = kwargs.get('lstm_output_dim', 8)\n",
    "        self.activation_lstm = kwargs.get('activation_lstm', 'relu')\n",
    "        self.activation_dense = kwargs.get('activation_dense', 'relu')\n",
    "        self.activation_last = kwargs.get('activation_last', 'softmax')    # softmax for multiple output\n",
    "        self.dense_layer = kwargs.get('dense_layer', 2)  # at least 2 layers\n",
    "        self.lstm_layer = kwargs.get('lstm_layer', 2) # at least 2 layers\n",
    "        self.drop_out = kwargs.get('drop_out', 0.2)\n",
    "        self.nb_epoch = kwargs.get('nb_epoch', 25)\n",
    "        self.batch_size = kwargs.get('batch_size', 100)\n",
    "        self.loss = kwargs.get('loss', 'categorical_crossentropy')\n",
    "        self.optimizer = kwargs.get('optimizer', 'rmsprop')\n",
    "\n",
    "\n",
    "    def NN_model_train(self, trainX, trainY, testX, testY, model_save_path):\n",
    "        \"\"\"\n",
    "        :param trainX: training data set\n",
    "        :param trainY: expect value of training data\n",
    "        :param testX: test data set\n",
    "        :param testY: expect value of test data\n",
    "        :param model_save_path: h5 file to store the trained model\n",
    "        :param override: override existing models\n",
    "        :return: model after training\n",
    "        \"\"\"\n",
    "        input_dim = trainX[0].shape[1]\n",
    "        output_dim = trainY.shape[1]\n",
    "        # print predefined parameters of current model:\n",
    "        model = Sequential()\n",
    "        # applying a LSTM layer with x dim output and y dim input. Use dropout parameter to avoid overfit\n",
    "        model.add(LSTM(output_dim=self.lstm_output_dim,\n",
    "                       input_dim=input_dim,\n",
    "                       activation=self.activation_lstm,\n",
    "                       dropout_U=self.drop_out,\n",
    "                       return_sequences=True))\n",
    "        for i in range(self.lstm_layer-2):\n",
    "            model.add(LSTM(output_dim=self.lstm_output_dim,\n",
    "                       activation=self.activation_lstm,\n",
    "                       dropout_U=self.drop_out,\n",
    "                       return_sequences=True ))\n",
    "        # return sequences should be False to avoid dim error when concatenating with dense layer\n",
    "        model.add(LSTM(output_dim=self.lstm_output_dim, activation=self.activation_lstm, dropout_U=self.drop_out))\n",
    "        # applying a full connected NN to accept output from LSTM layer\n",
    "        for i in range(self.dense_layer-1):\n",
    "            model.add(Dense(output_dim=self.lstm_output_dim, activation=self.activation_dense))\n",
    "            model.add(Dropout(self.drop_out))\n",
    "        model.add(Dense(output_dim=output_dim, activation=self.activation_last))\n",
    "        # configure the learning process\n",
    "        model.compile(loss=self.loss, optimizer=self.optimizer, metrics=['accuracy'])\n",
    "        # train the model with fixed number of epoches\n",
    "        model.fit(x=trainX, y=trainY, nb_epoch=self.nb_epoch, batch_size=self.batch_size, validation_data=(testX, testY))\n",
    "        score = model.evaluate(trainX, trainY, self.batch_size)\n",
    "        print (\"Model evaluation: {}\".format(score))\n",
    "        # store model to json file\n",
    "        save_model(model, model_save_path)\n",
    "    \n",
    "    def NN_prediction(dataset, model_save_path):\n",
    "        dataset = np.asarray(dataset)\n",
    "        if not os.path.exists(model_save_path):\n",
    "            raise ValueError(\"Lstm model not found! Train one first or check your input path: {}\".format(model_save_path))\n",
    "        model = load_model(model_save_path)\n",
    "        predict_class = model.predict_classes(dataset)\n",
    "        class_prob = model.predict_proba(dataset)\n",
    "        return predict_class, class_prob\n",
    "    \n",
    "    def model_train_predict_test(self, input_file_regx=\"^(\\d+)\\.csv\", override=False):\n",
    "        \"\"\"\n",
    "        :param override=Fasle: rerun the model prediction no matter if the expected output file exists\n",
    "        :return: model file, model weights files, prediction file, discrepancy statistic bar plot file\n",
    "        \"\"\"\n",
    "        # get training sets for lstm training\n",
    "        print (\"Scanning files within select id range ...\")\n",
    "        ids, files = get_ids_and_files_in_dir(inputdir=self.training_set_dir,\n",
    "                                                          range=self.training_set_id_range,\n",
    "                                                          input_file_regx=input_file_regx)\n",
    "        print (\"Scanning done! Selected enterprise ids are {}\".format(ids))\n",
    "        if not files:\n",
    "            raise ValueError(\"No files selected in current id range. Please check the input training set directory, \"\n",
    "                             \"input enterprise id range or file format which should be '[0-9]+.csv'\")\n",
    "\n",
    "        # get train, test, validation data\n",
    "        for id_index, id_file in enumerate(files):\n",
    "            # store prediction result to prediction directory\n",
    "            enter_file = self.training_set_dir + \"/\" + id_file\n",
    "            print (\"Processing dataset - enterprise_id is: {}\".format(ids[id_index]))\n",
    "            print (\"Reading from file {}\".format(enter_file))\n",
    "            df = pd.read_csv(enter_file)\n",
    "            df.index = range(len(df.index))\n",
    "            # retrieve training X and Y columns. First column is customer_id\n",
    "            select_col = ['customer_id']\n",
    "            select_col = np.append(select_col, ['X' + str(i) for i in range(1, 1+self.training_set_length)])\n",
    "            select_col = np.append(select_col, ['Y', 'enterprise_id'])\n",
    "            df_selected = df.ix[:, select_col]\n",
    "            # remove outlier records\n",
    "            df_selected = percentile_remove_outlier(df_selected, filter_start=1, filter_end=2+self.training_set_length)\n",
    "            # scale the train columns\n",
    "            print (\"Scaling...\")\n",
    "            if self.scaler == 'mm':\n",
    "                df_scale, minVal, maxVal = MinMaxScaler(df_selected, start_col_index=1, end_col_index=self.training_set_length+1)\n",
    "            elif self.scaler == 'norm':\n",
    "                df_scale, meanVal, stdVal = NormalDistributionScaler(df_selected, start_col_index=1, end_col_index=self.training_set_length+1)\n",
    "            else:\n",
    "                raise ValueError(\"Argument scaler must be mm or norm!\")\n",
    "            # bin date y\n",
    "            df_bin, bin_boundary = binning_date_y(df_scale, y_col=1+self.training_set_length, n_group=5)\n",
    "            print (\"Bin boundary is {}\".format(bin_boundary))\n",
    "            # get train and test dataset\n",
    "            print (\"Randomly selecting training set and test set...\")\n",
    "            all_data_x = np.asarray(df_bin.ix[:, 1:1+self.training_set_length]).reshape((len(df_bin.index), 1, self.training_set_length))\n",
    "            all_data_y = np.asarray(df_bin.ix[:, 1+self.training_set_length])\n",
    "            # convert y label to one-hot dummy label\n",
    "            y_dummy_label = np.asarray(pd.get_dummies(all_data_y))\n",
    "            # format train, test, validation data\n",
    "            sub_train, val_train, sub_test, val_test = train_test_split(all_data_x, y_dummy_label, test_size=self.test_size)\n",
    "            train_x, test_x, train_y, test_y = train_test_split(sub_train, sub_test, test_size=self.test_size)\n",
    "            # create and fit the NN model\n",
    "            model_save_path = self.model_save_dir + \"/\" + self.model_file_prefix + \"-\" + str(ids[id_index]) + \".h5\"\n",
    "            # check if model file exists\n",
    "            if not os.path.exists(model_save_path) or override:\n",
    "                self.NN_model_train(train_x, train_y, test_x, test_y, model_save_path=model_save_path)\n",
    "            # generate prediction for training\n",
    "            print (\"Predicting the output of validation set...\")\n",
    "            val_predict_class, val_predict_prob = self.NN_prediction(val_train, model_save_path=model_save_path)\n",
    "            # statistic of discrepancy between expected value and real value\n",
    "            total_sample_count = len(val_predict_class)\n",
    "            val_test_label = np.asarray([list(x).index(1) for x in val_test])\n",
    "            match_count = (np.asarray(val_predict_class) == np.asarray(val_test_label.ravel())).sum()\n",
    "            print (\"Precision using validation dataset is {}\".format(float(match_count) / total_sample_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3645dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
